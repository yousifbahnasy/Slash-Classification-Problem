{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a841dccf-4539-49c8-94e4-cfbe2b50af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec98a271-01b1-431c-95da-6bf97b4a4575",
   "metadata": {},
   "outputs": [],
   "source": [
    "accessories=r\"C:\\Users\\yousif\\OneDrive\\Desktop\\slash dataset\\accessories\"\n",
    "fashion=r\"C:\\Users\\yousif\\OneDrive\\Desktop\\slash dataset\\fashion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "841c2b23-5cfd-4f86-99ec-47d43444d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "accessories_output=r\"C:\\Users\\yousif\\OneDrive\\Desktop\\output\\accessories\"\n",
    "fashion_output=r\"C:\\Users\\yousif\\OneDrive\\Desktop\\output\\fashion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29f55238-5523-4f7b-8a01-28bd9b58c950",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_width0, crop_height0, crop_width1, crop_height1 = 0, 460, 550, 950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "032b78fd-a716-47f5-9538-c10119c19353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_data(output_path,dataset_path,crop_width0, crop_height0, crop_width1, crop_height1):\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        image_path=os.path.join(dataset_path,filename)\n",
    "        img=Image.open(image_path)\n",
    "        cropped_img=img.crop((crop_width0, crop_height0, crop_width1, crop_height1))\n",
    "        output_image_path=os.path.join(output_path,filename)\n",
    "        cropped_img.save(output_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3677bdd4-36d4-495d-8f34-0800c6b635e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_data(accessories_output,accessories,crop_width0, crop_height0, crop_width1, crop_height1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af891172-593d-4f6a-937c-1ebec483a301",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_data(fashion_output,fashion,crop_width0, crop_height0, crop_width1, crop_height1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffb22602-3a4c-4184-97b7-97f42d9780b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path=r\"C:\\Users\\yousif\\OneDrive\\Desktop\\output\"\n",
    "\n",
    "categories=[\"accessories\",\"fashion\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9a7a45e-1da2-4608-aa62-f9ceb809c9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splits = {'train': {}, 'validation': {}, 'test': {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee41715a-296c-4148-b978-ad144f2099bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    category_path=os.path.join(dataset_path,category)\n",
    "    images=os.listdir(category_path)\n",
    "    \n",
    "    train_images,temp_images=train_test_split(images,test_size=0.2,random_state=42)\n",
    "    val_images,test_images=train_test_split(temp_images,test_size=0.5,random_state=42)\n",
    "    \n",
    "    data_splits['train'][category] = train_images\n",
    "    data_splits['validation'][category] = val_images\n",
    "    data_splits['test'][category] = test_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ac11a20-b2b5-48ed-a2d4-6649729f01f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Split:\n",
      "accessories: 86 images\n",
      "fashion: 83 images\n",
      "\n",
      "Validation Split:\n",
      "accessories: 11 images\n",
      "fashion: 10 images\n",
      "\n",
      "Test Split:\n",
      "accessories: 11 images\n",
      "fashion: 11 images\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for split, categories_dict in data_splits.items():\n",
    "    print(f\"{split.capitalize()} Split:\")\n",
    "    for category, images in categories_dict.items():\n",
    "        print(f\"{category}: {len(images)} images\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61765a72-70d6-4ab6-9a50-8fd2555ca81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir      =  r\"C:\\Users\\yousif\\OneDrive\\Desktop\\dataset_modified\\train\"\n",
    "validation_dir = r\"C:\\Users\\yousif\\OneDrive\\Desktop\\dataset_modified\\val\"\n",
    "test_dir       = r\"C:\\Users\\yousif\\OneDrive\\Desktop\\dataset_modified\\test\"\n",
    "\n",
    "\n",
    "\n",
    "for split, categories_dict in data_splits.items():\n",
    "    for category, images in categories_dict.items():\n",
    "        category_dir=os.path.join(eval(f\"{split}_dir\"),category)\n",
    "        os.makedirs(category_dir,exist_ok=True)\n",
    "        \n",
    "        for image in images:\n",
    "            src=os.path.join(dataset_path,category,image)\n",
    "            dst=os.path.join(category_dir,image)\n",
    "            shutil.move(src,dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45121dd4-762f-4838-9fd9-0f8a6d495afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir=r\"C:\\Users\\yousif\\OneDrive\\Desktop\\dataset_modified\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a07236f2-d040-4d6a-afcc-9474bc7813ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Filepath     Category  Split\n",
      "0    C:\\Users\\yousif\\OneDrive\\Desktop\\dataset_modif...  accessories  train\n",
      "1    C:\\Users\\yousif\\OneDrive\\Desktop\\dataset_modif...  accessories  train\n",
      "2    C:\\Users\\yousif\\OneDrive\\Desktop\\dataset_modif...  accessories  train\n",
      "3    C:\\Users\\yousif\\OneDrive\\Desktop\\dataset_modif...  accessories  train\n",
      "4    C:\\Users\\yousif\\OneDrive\\Desktop\\dataset_modif...  accessories  train\n",
      "..                                                 ...          ...    ...\n",
      "212  C:\\Users\\yousif\\OneDrive\\Desktop\\dataset_modif...      fashion   test\n",
      "213  C:\\Users\\yousif\\OneDrive\\Desktop\\dataset_modif...      fashion   test\n",
      "214  C:\\Users\\yousif\\OneDrive\\Desktop\\dataset_modif...      fashion   test\n",
      "215  C:\\Users\\yousif\\OneDrive\\Desktop\\dataset_modif...      fashion   test\n",
      "216  C:\\Users\\yousif\\OneDrive\\Desktop\\dataset_modif...      fashion   test\n",
      "\n",
      "[217 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "filepaths = []\n",
    "categories = []\n",
    "splits = []\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    split_dir = os.path.join(dataset_dir, split)\n",
    "    \n",
    "    for category in os.listdir(split_dir):\n",
    "        category_dir = os.path.join(split_dir, category)\n",
    "        \n",
    "        # This loop should be nested within the loop that iterates over categories\n",
    "        for filename in os.listdir(category_dir):\n",
    "            filepath = os.path.join(category_dir, filename)\n",
    "            filepaths.append(filepath)\n",
    "            categories.append(category)\n",
    "            splits.append(split)\n",
    "\n",
    "data = {\n",
    "    'Filepath': filepaths,\n",
    "    'Category': categories,\n",
    "    'Split': splits\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7cae54a5-213d-4ca6-b554-53e3f03359e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Filepath Category  Split\n",
      "88   C:\\Users\\yousif\\OneDrive\\Desktop\\dataset_modif...  fashion  train\n",
      "89   C:\\Users\\yousif\\OneDrive\\Desktop\\dataset_modif...  fashion  train\n",
      "90   C:\\Users\\yousif\\OneDrive\\Desktop\\dataset_modif...  fashion  train\n",
      "91   C:\\Users\\yousif\\OneDrive\\Desktop\\dataset_modif...  fashion  train\n",
      "92   C:\\Users\\yousif\\OneDrive\\Desktop\\dataset_modif...  fashion  train\n",
      "..                                                 ...      ...    ...\n",
      "212  C:\\Users\\yousif\\OneDrive\\Desktop\\dataset_modif...  fashion   test\n",
      "213  C:\\Users\\yousif\\OneDrive\\Desktop\\dataset_modif...  fashion   test\n",
      "214  C:\\Users\\yousif\\OneDrive\\Desktop\\dataset_modif...  fashion   test\n",
      "215  C:\\Users\\yousif\\OneDrive\\Desktop\\dataset_modif...  fashion   test\n",
      "216  C:\\Users\\yousif\\OneDrive\\Desktop\\dataset_modif...  fashion   test\n",
      "\n",
      "[107 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame to show only entries where the category is \"accessory\"\n",
    "accessory_df = df[df['Category'] == 'fashion']\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(accessory_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5a95beae-3d54-4fc8-a4f2-c960a11213f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=df[df[\"Split\"]==\"train\"]\n",
    "test_df=df[df[\"Split\"]==\"test\"]\n",
    "val_df=df[df[\"Split\"]==\"val\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "11128438-7d6e-4f79-a5ce-5f783f100bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_generators(train_df,test_df,val_df):\n",
    "    train_generator=tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=True,\n",
    "                                                                    rescale=1./255,\n",
    "                                                                    preprocessing_function=tf.keras.applications.resnet50.preprocess_input)\n",
    "    \n",
    "    test_generator=tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n",
    "                                                                  preprocessing_function=tf.keras.applications.resnet50.preprocess_input)\n",
    "    \n",
    "    val_generator=tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n",
    "                                                                 preprocessing_function=tf.keras.applications.resnet50.preprocess_input)\n",
    "    \n",
    "    train_images = train_generator.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        x_col='Filepath',\n",
    "        y_col='Category',\n",
    "        target_size=(224, 224),\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "        seed=42,\n",
    "    )\n",
    "    \n",
    "\n",
    "    val_images = val_generator.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        x_col='Filepath',\n",
    "        y_col='Category',\n",
    "        target_size=(224, 224),\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "        seed=42\n",
    "    )\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    test_images = test_generator.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        x_col='Filepath',\n",
    "        y_col='Category',\n",
    "        target_size=(224, 224),\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "        seed=42\n",
    "    )\n",
    "    return train_images, val_images, test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65e49ab8-4f36-4eb7-8014-485f08dd13f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # Build and return the ResNet50-based model\n",
    "    pretrained_model = tf.keras.applications.resnet50.ResNet50(\n",
    "        input_shape=(224, 224, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg'\n",
    "    )\n",
    "\n",
    "    pretrained_model.trainable = False\n",
    "\n",
    "    inputs = pretrained_model.input\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\n",
    "    x = tf.keras.layers.Dense(50, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a8d6bd8d-4492-4bcc-8185-fd22d0d8ba73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_slash_model(train_images, val_images, test_images, save_path=\"slash_model.h5\"):\n",
    "    model = build_model()\n",
    "    print(\"-------Training Slash Model-------\")\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    history = model.fit(train_images, validation_data=val_images, epochs=25, callbacks=[callbacks])\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    print(\"-------Evaluating Slash Model-------\")\n",
    "    test_loss, test_accuracy = model.evaluate(test_images, verbose=1)\n",
    "    print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    # Save the model\n",
    "    print(\"-------Saving Model-------\")\n",
    "    model.save(save_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "de92182d-db74-4975-b4ce-569c2652df50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 171 validated image filenames belonging to 2 classes.\n",
      "Found 171 validated image filenames belonging to 2 classes.\n",
      "Found 171 validated image filenames belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yousif\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py:1137: UserWarning: Found 3 invalid image filename(s) in x_col=\"Filepath\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yousif\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py:1137: UserWarning: Found 3 invalid image filename(s) in x_col=\"Filepath\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yousif\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py:1137: UserWarning: Found 3 invalid image filename(s) in x_col=\"Filepath\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Training Slash Model-------\n",
      "Epoch 1/25\n",
      "3/3 [==============================] - 24s 9s/step - loss: 0.7146 - accuracy: 0.4094 - val_loss: 0.7022 - val_accuracy: 0.4971\n",
      "Epoch 2/25\n",
      "3/3 [==============================] - 19s 8s/step - loss: 0.7104 - accuracy: 0.4854 - val_loss: 0.7015 - val_accuracy: 0.4971\n",
      "Epoch 3/25\n",
      "3/3 [==============================] - 18s 7s/step - loss: 0.6985 - accuracy: 0.4971 - val_loss: 0.6872 - val_accuracy: 0.5029\n",
      "Epoch 4/25\n",
      "3/3 [==============================] - 19s 8s/step - loss: 0.6839 - accuracy: 0.5322 - val_loss: 0.6782 - val_accuracy: 0.7076\n",
      "Epoch 5/25\n",
      "3/3 [==============================] - 19s 8s/step - loss: 0.6812 - accuracy: 0.6491 - val_loss: 0.6748 - val_accuracy: 0.6257\n",
      "Epoch 6/25\n",
      "3/3 [==============================] - 18s 7s/step - loss: 0.6744 - accuracy: 0.6257 - val_loss: 0.6687 - val_accuracy: 0.6374\n",
      "Epoch 7/25\n",
      "3/3 [==============================] - 18s 7s/step - loss: 0.6666 - accuracy: 0.6667 - val_loss: 0.6611 - val_accuracy: 0.7661\n",
      "Epoch 8/25\n",
      "3/3 [==============================] - 18s 8s/step - loss: 0.6634 - accuracy: 0.6082 - val_loss: 0.6588 - val_accuracy: 0.5439\n",
      "Epoch 9/25\n",
      "3/3 [==============================] - 19s 7s/step - loss: 0.6576 - accuracy: 0.5789 - val_loss: 0.6533 - val_accuracy: 0.6433\n",
      "Epoch 10/25\n",
      "3/3 [==============================] - 19s 7s/step - loss: 0.6515 - accuracy: 0.6199 - val_loss: 0.6477 - val_accuracy: 0.7310\n",
      "Epoch 11/25\n",
      "3/3 [==============================] - 19s 8s/step - loss: 0.6450 - accuracy: 0.7427 - val_loss: 0.6429 - val_accuracy: 0.7836\n",
      "Epoch 12/25\n",
      "3/3 [==============================] - 18s 7s/step - loss: 0.6403 - accuracy: 0.7310 - val_loss: 0.6394 - val_accuracy: 0.6901\n",
      "Epoch 13/25\n",
      "3/3 [==============================] - 19s 7s/step - loss: 0.6370 - accuracy: 0.6901 - val_loss: 0.6356 - val_accuracy: 0.6842\n",
      "Epoch 14/25\n",
      "3/3 [==============================] - 23s 9s/step - loss: 0.6322 - accuracy: 0.7193 - val_loss: 0.6303 - val_accuracy: 0.7485\n",
      "Epoch 15/25\n",
      "3/3 [==============================] - 20s 8s/step - loss: 0.6284 - accuracy: 0.7602 - val_loss: 0.6262 - val_accuracy: 0.7602\n",
      "Epoch 16/25\n",
      "3/3 [==============================] - 19s 8s/step - loss: 0.6238 - accuracy: 0.7485 - val_loss: 0.6221 - val_accuracy: 0.7661\n",
      "Epoch 17/25\n",
      "3/3 [==============================] - 19s 8s/step - loss: 0.6204 - accuracy: 0.7427 - val_loss: 0.6189 - val_accuracy: 0.7018\n",
      "Epoch 18/25\n",
      "3/3 [==============================] - 19s 7s/step - loss: 0.6155 - accuracy: 0.7135 - val_loss: 0.6157 - val_accuracy: 0.7135\n",
      "Epoch 19/25\n",
      "3/3 [==============================] - 18s 8s/step - loss: 0.6107 - accuracy: 0.7368 - val_loss: 0.6111 - val_accuracy: 0.7836\n",
      "Epoch 20/25\n",
      "3/3 [==============================] - 19s 7s/step - loss: 0.6066 - accuracy: 0.7719 - val_loss: 0.6081 - val_accuracy: 0.7778\n",
      "Epoch 21/25\n",
      "3/3 [==============================] - 19s 7s/step - loss: 0.6055 - accuracy: 0.7719 - val_loss: 0.6051 - val_accuracy: 0.7602\n",
      "Epoch 22/25\n",
      "3/3 [==============================] - 18s 7s/step - loss: 0.6006 - accuracy: 0.7544 - val_loss: 0.6007 - val_accuracy: 0.7778\n",
      "Epoch 23/25\n",
      "3/3 [==============================] - 19s 8s/step - loss: 0.5972 - accuracy: 0.7485 - val_loss: 0.5974 - val_accuracy: 0.7368\n",
      "Epoch 24/25\n",
      "3/3 [==============================] - 19s 8s/step - loss: 0.5946 - accuracy: 0.7485 - val_loss: 0.5937 - val_accuracy: 0.7427\n",
      "Epoch 25/25\n",
      "3/3 [==============================] - 18s 8s/step - loss: 0.5900 - accuracy: 0.7661 - val_loss: 0.5902 - val_accuracy: 0.7778\n",
      "-------Evaluating Slash Model-------\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.5902 - accuracy: 0.7778\n",
      "Test Loss: 0.5901924967765808, Test Accuracy: 0.7777777910232544\n",
      "-------Saving Model-------\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Create image generators\n",
    "    train_images, val_images, test_images = create_image_generators(train_df, val_df, test_df)\n",
    "\n",
    "    # Train and evaluate the Slash model\n",
    "    model = train_and_evaluate_slash_model(train_images, val_images, test_images)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a8e842-fd63-4008-98f4-2271f65185b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
